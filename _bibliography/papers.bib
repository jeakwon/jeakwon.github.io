---
---

@inproceedings{kim2022nmdar,
  abbr={NeurIPS-W},
	author = {Kim, Dong-Kyum and Kwon, Jea and Cha, Meeyoung and C. Justin Lee},
	title = {Transformer needs NMDA receptor nonlinearity for long-term memory},
	year = {2022},
	abstract = {The NMDA receptor (NMDAR) in the hippocampus is essential for learning and memory. We find an interesting resemblance between deep models' nonlinear activation function and the NMDAR's nonlinear dynamics. In light of a recent study that compared the transformer architecture to the formation of hippocampal memory, this paper presents new findings that NMDAR-like nonlinearity may be essential for consolidating short-term working memory into long-term reference memory. We design a navigation task assessing these two memory functions and show that manipulating the activation function (i.e., mimicking the Mg$^{2+}$-gating of NMDAR) disrupts long-term memory formation. Our experimental data suggest that the concept of place cells and reference memory may reside in the feed-forward network and that nonlinearity plays a key role in these processes. Our findings propose that the transformer architecture and hippocampal spatial representation resemble by sharing the overlapping concept of NMDAR nonlinearity.},
  booktitle={NeurIPS 2022 Memory in Artificial and Real Intelligence workshop},
  url={https://openreview.net/forum?id=BJtzDKdXDW},
  pdf={transformer_needs_nmdar.pdf},
}

@article{kim2020multi,
  abbr = {JKPS},
  bibtex_show={true},
  title={Multi-label classification of historical documents by using hierarchical attention networks},
  author={Kim, Dong-Kyum and Lee, Byunghwee and Kim, Daniel and Jeong, Hawoong},
  journal={Journal of the Korean Physical Society},
  volume={76},
  number={5},
  pages={368--377},
  year={2020},
  publisher={Springer},
  html = {https://link.springer.com/article/10.3938/jkps.76.368},
  abstract={The quantitative analysis of digitized historical documents has begun in earnest in recent years. Text classification is of particular importance for quantitative historical analysis because it helps to search literature efficiently and to determine the important subjects of a particular age. While numerous historians have joined together to classify large-scale historical documents, consistent classification among individual researchers has not been achieved. In this study, we present a classification method for large-scale historical data that uses a recently developed supervised learning algorithm called the Hierarchical Attention Network (HAN). By applying various classification methods to the Annals of the Joseon Dynasty (AJD), we show that HAN is more accurate than conventional techniques with word-frequency-based features. HAN provides the extent that a particular sentence or word contributes to the classification process through a quantitative value called ’attention’. We extract the representative keywords from various categories by using the attention mechanism and show the evolution of the keywords over the 472-year span of the AJD. Our results reveal that largely two groups of event categories are found in the AJD. In one group, the representative keywords of the categories were stable over long periods while the keywords in the other group varied rapidly, exhibiting repeatedly changing characteristics of the categories. Observing such macroscopic changes of representative words may provide insight into how a particular topic changes over a historical period.}
}

@article{han2019tweety,
  title={Tweety-homolog (Ttyh) family encodes the pore-forming subunits of the swelling-dependent volume-regulated anion channel (VRACswell) in the brain},
  author={Han, Young-Eun and Kwon, Jea and Won, Joungha and An, Heeyoung and Jang, Minwoo Wendy and Woo, Junsung and Lee, Je Sun and Park, Min Gu and Yoon, Bo-Eun and Lee, Seung Eun and others},
  journal={Experimental neurobiology},
  volume={28},
  number={2},
  pages={183},
  year={2019},
  publisher={Korean Society for Brain and Neural Science}
}

@article{woo2018control,
  title={Control of motor coordination by astrocytic tonic GABA release through modulation of excitation/inhibition balance in cerebellum},
  author={Woo, Junsung and Min, Joo Ok and Kang, Dae-Si and Kim, Yoo Sung and Jung, Guk Hwa and Park, Hyun Jung and Kim, Sunpil and An, Heeyoung and Kwon, Jea and Kim, Jeongyeon and others},
  journal={Proceedings of the National Academy of Sciences},
  volume={115},
  number={19},
  pages={5004--5009},
  year={2018},
  publisher={National Acad Sciences}
}

@article{kwon2018development,
  title={Development of a Low-cost, Comprehensive Recording System for Circadian Rhythm Behavior},
  author={Kwon, Jea and Park, Min Gu and Lee, Seung Eun and Lee, C Justin},
  journal={Experimental Neurobiology},
  volume={27},
  number={1},
  pages={65--75},
  year={2018},
  publisher={The Korean Society for Brain and Neural Science}
}

@article{kwon2017orai1,
  abbr={EN},
  title={Orai1 and Orai3 in combination with Stim1 mediate the majority of store-operated calcium entry in astrocytes},
  author={Kwon, Jea and An, Heeyoung and Sa, Moonsun and Won, Joungha and Im Shin, Jeong and Lee, C Justin},
  journal={Experimental Neurobiology},
  volume={26},
  number={1},
  pages={42},
  year={2017},
  publisher={Korean Society for Brain and Neural Science}
}

@article{ha2016ca2+,
  abbr={EN},
  title={The Ca2+-activated chloride channel anoctamin-2 mediates spike-frequency adaptation and regulates sensory transmission in thalamocortical neurons},
  author={Ha, Go Eun and Lee, Jaekwang and Kwak, Hankyul and Song, Kiyeong and Kwon, Jea and Jung, Soon-Young and Hong, Joohyeon and Chang, Gyeong-Eon and Hwang, Eun Mi and Shin, Hee-Sup and others},
  journal={Nature communications},
  volume={7},
  number={1},
  pages={1--13},
  year={2016},
  publisher={Nature Publishing Group}

  abbr={NeurIPS-W},
	author = {Kim, Dong-Kyum and Kwon, Jea and Cha, Meeyoung and C. Justin Lee},
	title = {Transformer needs NMDA receptor nonlinearity for long-term memory},
	year = {2022},
	abstract = {The NMDA receptor (NMDAR) in the hippocampus is essential for learning and memory. We find an interesting resemblance between deep models' nonlinear activation function and the NMDAR's nonlinear dynamics. In light of a recent study that compared the transformer architecture to the formation of hippocampal memory, this paper presents new findings that NMDAR-like nonlinearity may be essential for consolidating short-term working memory into long-term reference memory. We design a navigation task assessing these two memory functions and show that manipulating the activation function (i.e., mimicking the Mg$^{2+}$-gating of NMDAR) disrupts long-term memory formation. Our experimental data suggest that the concept of place cells and reference memory may reside in the feed-forward network and that nonlinearity plays a key role in these processes. Our findings propose that the transformer architecture and hippocampal spatial representation resemble by sharing the overlapping concept of NMDAR nonlinearity.},
  booktitle={NeurIPS 2022 Memory in Artificial and Real Intelligence workshop},
  url={https://openreview.net/forum?id=BJtzDKdXDW},
  pdf={transformer_needs_nmdar.pdf},

}
